{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_csv(file, idxname):\n",
    "#     data = pd.read_csv(file, delimiter=\"\\t\", header=None).reset_index().rename(columns = {0: \"tokens\", \"index\": idxname}).set_index(\"tokens\")\n",
    "#     return data\n",
    "\n",
    "# nfpa_en_vocab = read_csv(\"/home/ubuntu/cliang/train/pipeline_test_v2/preprocessed_data/NFPA_CS_Train_Vocab.en\", \"nfpa_idx\")\n",
    "# euro_en_vocab = read_csv(\"/home/ubuntu/cliang/train/pipeline_test_v2/preprocessed_data/Europarl_Train_Vocab.en\", \"euro_idx\")\n",
    "# merge_domain_vocab = read_csv(\"/home/ubuntu/cliang/train/pipeline_test_v2/domain_model/encoder-merge-NFPA_CS_Train_Vocab.en\", \"merge_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_exist(token_list, model_dir=\"/home/ubuntu/cliang/train/pipeline_test_v2\", language=\"en\", check_tag = False):\n",
    "    # check if this word show up in the vocab file\n",
    "    # :params: model_dir: directory to the model\n",
    "    # :params: language: the language of the input token list\n",
    "    # :params: check_tag: if check whether this word belongs to the nfpa/euro vocab, if not, only check whether this word belongs to share vocab\n",
    "    if check_tag == True:\n",
    "        merge_vocab = {\"en\": pd.read_csv(os.path.join(model_dir, \"domain_model/encoder-merge-NFPA_CS_Train_Vocab.en\"), delimiter=\"\\t\", header=None) \\\n",
    "            .rename(columns = {0: \"tokens\"}).set_index(\"tokens\"), \\\n",
    "                       \"es\": pd.read_csv(os.path.join(model_dir, \"domain_model/decoder-merge-NFPA_CS_Train_Vocab.es\"), delimiter=\"\\t\", header=None) \\\n",
    "            .rename(columns = {0: \"tokens\"}).set_index(\"tokens\")}\n",
    "        nfpa_vocab = {\"en\": pd.read_csv(os.path.join(model_dir, \"preprocessed_data/NFPA_CS_Train_Vocab.en\"), delimiter=\"\\t\", header=None) \\\n",
    "            .rename(columns = {0: \"tokens\"}).set_index(\"tokens\"), \\\n",
    "                      \"es\": pd.read_csv(os.path.join(model_dir, \"preprocessed_data/NFPA_CS_Train_Vocab.es\"), delimiter=\"\\t\", header=None) \\\n",
    "            .rename(columns = {0: \"tokens\"}).set_index(\"tokens\")}\n",
    "        euro_vocab = {\"en\": pd.read_csv(os.path.join(model_dir, \"preprocessed_data/Europarl_Train_Vocab.en\"), delimiter=\"\\t\", header=None) \\\n",
    "            .rename(columns = {0: \"tokens\"}).set_index(\"tokens\"), \\\n",
    "                      \"es\": pd.read_csv(os.path.join(model_dir, \"preprocessed_data/Europarl_Train_Vocab.es\"), delimiter=\"\\t\", header=None) \\\n",
    "            .rename(columns = {0: \"tokens\"}).set_index(\"tokens\")}\n",
    "        token_tag_dict = {token: (token in merge_vocab[language].index, \\\n",
    "                                  token in nfpa_vocab[language].index, \\\n",
    "                                  token in euro_vocab[language].index) for token in token_list}\n",
    "        token_tag_dict = {k: \"shared\" if all([v[1], v[2]]) else (\"nfpa\" if all([v[1], not v[2]]) else (\"euro\" if all([not v[1], v[2]]) else None)) for k,v in token_tag_dict.items()}\n",
    "        token_tag_dict_strip = {k.lstrip(\"_\"): v for (k,v) in token_tag_dict.items()}\n",
    "        nfpa = [k for k,v in token_tag_dict.items() if v == \"nfpa\"]\n",
    "        euro = [k for k,v in token_tag_dict.items() if v == \"euro\"]\n",
    "        share = [k for k,v in token_tag_dict.items() if v == \"shared\"]\n",
    "        oov = [k for k,v in token_tag_dict.items() if v == None]\n",
    "        result = {\"token_list\": token_tag_dict, \"striped_token_list\": token_tag_dict_strip, \"nfpa\": nfpa, \"euro\": euro, \"share\": share, \"oov\":oov}\n",
    "        return result\n",
    "    else:\n",
    "        merge_vocab = {\"en\": pd.read_csv(os.path.join(model_dir, \"domain_model/encoder-merge-NFPA_CS_Train_Vocab.en\"), delimiter=\"\\t\", header=None) \\\n",
    "            .reset_index().rename(columns = {0: \"tokens\", \"index\": \"merge_idx\"}).set_index(\"tokens\"), \\\n",
    "                       \"es\": pd.read_csv(os.path.join(model_dir, \"domain_model/decoder-merge-NFPA_CS_Train_Vocab.es\"), delimiter=\"\\t\", header=None) \\\n",
    "            .reset_index().rename(columns = {0: \"tokens\", \"index\": \"merge_idx\"}).set_index(\"tokens\")}\n",
    "        token_tag_dict = {token: token in merge_vocab[language].index for token in token_list}\n",
    "        token_tag_dict_strip = {k.lstrip(\"_\"): v for (k,v) in token_tag_dict.items()}\n",
    "        iv = [k for k,v in token_tag_dict.items() if v == True]\n",
    "        oov = [k for k,v in token_tag_dict.items() if v == False]\n",
    "        result = {\"token_list\": token_tag_dict, \"striped_token_list\": token_tag_dict_strip, \"iv\": iv, \"oov\": oov}\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_list': {'this': True,\n",
       "  'is': True,\n",
       "  '575': True,\n",
       "  'fire': True,\n",
       "  '__an': False,\n",
       "  '__apple.': False,\n",
       "  'decorum': False,\n",
       "  'misses': True},\n",
       " 'striped_token_list': {'this': True,\n",
       "  'is': True,\n",
       "  '575': True,\n",
       "  'fire': True,\n",
       "  'an': False,\n",
       "  'apple.': False,\n",
       "  'decorum': False,\n",
       "  'misses': True},\n",
       " 'iv': ['this', 'is', '575', 'fire', 'misses'],\n",
       " 'oov': ['__an', '__apple.', 'decorum']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_exist([\"this\", \"is\", \"575\", \"fire\", \"__an\", \"__apple.\", \"decorum\", \"misses\"], \\\n",
    "            \"/home/ubuntu/cliang/train/pipeline_test_v2\", \"en\", check_tag = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_list': {'this': 'shared',\n",
       "  'is': 'euro',\n",
       "  '575': 'nfpa',\n",
       "  'fire': 'shared',\n",
       "  '__an': None,\n",
       "  '__apple.': None,\n",
       "  'decorum': None,\n",
       "  'misses': None},\n",
       " 'striped_token_list': {'this': 'shared',\n",
       "  'is': 'euro',\n",
       "  '575': 'nfpa',\n",
       "  'fire': 'shared',\n",
       "  'an': None,\n",
       "  'apple.': None,\n",
       "  'decorum': None,\n",
       "  'misses': None},\n",
       " 'nfpa': ['575'],\n",
       " 'euro': ['is'],\n",
       " 'share': ['this', 'fire'],\n",
       " 'oov': ['__an', '__apple.', 'decorum', 'misses']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_exist([\"this\", \"is\", \"575\", \"fire\", \"__an\", \"__apple.\", \"decorum\", \"misses\"], \\\n",
    "            \"/home/ubuntu/cliang/train/pipeline_test_v2\", \"es\", check_tag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
